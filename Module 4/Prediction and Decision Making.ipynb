{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Making: Determining a Good Model Fit\n",
    "To determine final best fit, we look at combination of:\n",
    "- Do the predicted values make sense\n",
    "- Visualization\n",
    "- Numerical measures for evaluation\n",
    "- Comparing Models\n",
    "\n",
    "# Do the predicted values make sense\n",
    "- Train the model first:\n",
    "  - lm.fit(df['columnname1'],df['columnname2'])\n",
    "- Predict the price of a car with 30 highway-mpg.\n",
    "  - lm.predict(np.array(30.0).reshape(-1,1))\n",
    "- Result:\n",
    "  - lm.coef_\n",
    "\n",
    "- To generate a sequence of values in a specified range\n",
    "  1. import numpy as np\n",
    "  2. use numpy function arange to generate a sequence from 1 to 100: new_input = np.arange(1, 101, 1).reshape(-1,1)\n",
    "- To predict new values\n",
    "  - yhat = lm.predict(new_input)\n",
    "\n",
    "# Visualization\n",
    "- Visualize data with regression\n",
    "\n",
    "# Numerical measures for evaluation\n",
    "- As the square error increases the targets get further from the predicted points.\n",
    "-  r squared values range from zero to one.\n",
    "- An r squared of 1 means that all movements of another dependent variable are completely explained by movements in the independent variables.\n",
    "- An r squared of 0.9986 the model appears to be a good fit. That means that more than 99 of the variability of the predicted variable is explained by the independent variables. \n",
    "- An r squared of 0.806 of the data we can visually see that the values are scattered around the line. They are still close to the line and we can say that 80 percent of the variability of the predicted variable is explained by the independent variables. \n",
    "- And an r squared 0.61 means that approximately 61 percent of the observed variation can be explained by the independent variables. \n",
    "\n",
    "# Comparing MLR and SLR\n",
    "- Does a lower Mean Square Error imply better fit? No necessarily\n",
    "  1. Mean Square Error for a Multiple Linear Regression Model will be smaller than the Mean Square for a Simple Linear Regression model, since the errors of the data will decrease when more variables are included in the model.\n",
    "  2. Polynomial regression will also have a smaller Mean Square Error than the linear regualar regression.\n",
    "\n",
    " # Lesson Summary\n",
    "- Define the explanatory variable and the response variable: Define the response variable (y) as the focus of the experiment and the explanatory variable (x) as a variable used to explain the change of the response variable. Understand the differences between Simple Linear Regression because it concerns the study of only one explanatory variable and Multiple Linear Regression because it concerns the study of two or more explanatory variables.\n",
    "\n",
    "- Evaluate the model using Visualization: By visually representing the errors of a variable using scatterplots and interpreting the results of the model.\n",
    "\n",
    "- Identify alternative regression approaches: Use a Polynomial Regression when the Linear regression does not capture the curvilinear relationship between variables and how to pick the optimal order to use in a model.\n",
    "\n",
    "- Interpret the R-square and the Mean Square Error: Interpret R-square (x 100) as the percentage of the variation in the response variable y  that is explained by the variation in explanatory variable(s) x. The Mean Squared Error tells you how close a regression line is to a set of points. It does this by taking the average distances from the actual points to the predicted points and squaring them.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
